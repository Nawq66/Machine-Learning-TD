"""
Partie 3 : Analyse de corr√©lation via des tests statistiques
TD Machine Learning - IFP School
Tests statistiques rigoureux des relations entre variables
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import pearsonr, spearmanr, kendalltau
from scipy.stats import f_oneway, kruskal, chi2_contingency
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from statsmodels.stats.anova import anova_lm
from statsmodels.formula.api import ols
import warnings
warnings.filterwarnings('ignore')

# Configuration des graphiques
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")
plt.rcParams['figure.dpi'] = 100
plt.rcParams['savefig.dpi'] = 300

print("="*70)
print("PARTIE 3 : ANALYSE STATISTIQUE DES CORR√âLATIONS")
print("="*70)

# =====================================================
# CHARGEMENT DES DONN√âES
# =====================================================
print("\nüìÇ Chargement du dataset...")
df = pd.read_csv('eco2mix_cleaned.csv')
df['Datetime'] = pd.to_datetime(df['Datetime'])

print(f"‚úì Dataset charg√© : {df.shape[0]:,} lignes, {df.shape[1]} colonnes")

# =====================================================
# 3.1 TESTS DE CORR√âLATION ENTRE VARIABLES QUANTITATIVES
# =====================================================
print("\n" + "="*60)
print("3.1 CORR√âLATION ENTRE VARIABLES QUANTITATIVES")
print("="*60)

# S√©lection de paires de variables pertinentes pour l'analyse
pairs_to_test = [
    ('Consommation_MW', 'Production_totale_MW'),
    ('Consommation_MW', 'Thermique_MW'),
    ('Part_renouvelable', 'Nucleaire_MW'),
    ('Eolien_MW', 'Solaire_MW'),
    ('Hydraulique_MW', 'Balance_MW'),
    ('Hour', 'Solaire_MW'),
    ('Consommation_MW', 'Echanges_physiques_MW')
]

# Fonction pour r√©aliser tous les tests de corr√©lation
def comprehensive_correlation_test(x, y, var1_name, var2_name):
    """
    R√©alise une batterie compl√®te de tests de corr√©lation
    """
    # Nettoyer les donn√©es (enlever les NA)
    mask = ~(np.isnan(x) | np.isnan(y))
    x_clean = x[mask]
    y_clean = y[mask]
    
    results = {
        'Variables': f"{var1_name} vs {var2_name}",
        'N': len(x_clean)
    }
    
    # 1. Test de Pearson (corr√©lation lin√©aire)
    pearson_r, pearson_p = pearsonr(x_clean, y_clean)
    results['Pearson_r'] = pearson_r
    results['Pearson_p'] = pearson_p
    
    # 2. Test de Spearman (corr√©lation monotone)
    spearman_r, spearman_p = spearmanr(x_clean, y_clean)
    results['Spearman_rho'] = spearman_r
    results['Spearman_p'] = spearman_p
    
    # 3. Test de Kendall (corr√©lation ordinale)
    # Utiliser un √©chantillon si trop de donn√©es (calcul lourd)
    if len(x_clean) > 5000:
        indices = np.random.choice(len(x_clean), 5000, replace=False)
        x_sample = x_clean[indices]
        y_sample = y_clean[indices]
    else:
        x_sample = x_clean
        y_sample = y_clean
    
    kendall_tau, kendall_p = kendalltau(x_sample, y_sample)
    results['Kendall_tau'] = kendall_tau
    results['Kendall_p'] = kendall_p
    
    # 4. Test de lin√©arit√© (R¬≤)
    results['R_squared'] = pearson_r ** 2
    
    # 5. Test de normalit√© des r√©sidus (pour validit√© de Pearson)
    # R√©gression simple
    z = np.polyfit(x_clean, y_clean, 1)
    p = np.poly1d(z)
    residuals = y_clean - p(x_clean)
    
    # Test de Shapiro-Wilk sur √©chantillon
    if len(residuals) > 5000:
        residuals_sample = np.random.choice(residuals, 5000, replace=False)
    else:
        residuals_sample = residuals
    
    shapiro_stat, shapiro_p = stats.shapiro(residuals_sample)
    results['Residuals_normal'] = shapiro_p > 0.05
    
    return results

# Tableau pour stocker les r√©sultats
correlation_results = []

# Figure pour visualiser les relations
fig1, axes = plt.subplots(3, 3, figsize=(18, 16))
fig1.suptitle('Analyse des corr√©lations entre variables quantitatives', fontsize=16)

# R√©aliser les tests pour chaque paire
for idx, (var1, var2) in enumerate(pairs_to_test):
    if idx < 9:  # Limiter aux 9 premiers pour la visualisation
        ax = axes[idx//3, idx%3]
        
        # √âchantillonner pour la visualisation si trop de points
        if len(df) > 10000:
            sample_idx = np.random.choice(df.index, 10000, replace=False)
            df_sample = df.loc[sample_idx]
        else:
            df_sample = df
        
        # Scatter plot avec r√©gression
        x_data = df[var1].values
        y_data = df[var2].values
        
        # R√©sultats des tests
        results = comprehensive_correlation_test(x_data, y_data, var1, var2)
        correlation_results.append(results)
        
        # Visualisation
        ax.scatter(df_sample[var1], df_sample[var2], alpha=0.3, s=1)
        
        # Ligne de r√©gression
        mask = ~(np.isnan(x_data) | np.isnan(y_data))
        z = np.polyfit(x_data[mask], y_data[mask], 1)
        p = np.poly1d(z)
        x_line = np.linspace(np.nanmin(x_data), np.nanmax(x_data), 100)
        ax.plot(x_line, p(x_line), "r-", linewidth=2, label='R√©gression lin√©aire')
        
        # Ajouter les statistiques sur le graphique
        textstr = f'Pearson r = {results["Pearson_r"]:.3f}***\n'
        textstr += f'Spearman œÅ = {results["Spearman_rho"]:.3f}***\n'
        textstr += f'R¬≤ = {results["R_squared"]:.3f}'
        
        # Signification statistique
        if results["Pearson_p"] < 0.001:
            sig = '***'
        elif results["Pearson_p"] < 0.01:
            sig = '**'
        elif results["Pearson_p"] < 0.05:
            sig = '*'
        else:
            sig = 'ns'
        
        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
        ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=9,
                verticalalignment='top', bbox=props)
        
        ax.set_xlabel(var1.replace('_', ' '))
        ax.set_ylabel(var2.replace('_', ' '))
        ax.set_title(f'{var1.split("_")[0]} vs {var2.split("_")[0]}', fontweight='bold')

# Ajuster les sous-graphiques vides
for idx in range(len(pairs_to_test), 9):
    ax = axes[idx//3, idx%3]
    ax.axis('off')

plt.tight_layout()
plt.savefig('correlation_analysis_quantitative.png', bbox_inches='tight')
plt.show()

# Cr√©er un DataFrame avec les r√©sultats
results_df = pd.DataFrame(correlation_results)

print("\nüìä R√âSULTATS DES TESTS DE CORR√âLATION (Variables Quantitatives)")
print("="*80)
print(results_df.to_string(index=False))

# Interpr√©tation des forces de corr√©lation
def interpret_correlation(r):
    """Interpr√®te la force d'une corr√©lation selon les standards Cohen"""
    r_abs = abs(r)
    if r_abs < 0.1:
        return "N√©gligeable"
    elif r_abs < 0.3:
        return "Faible"
    elif r_abs < 0.5:
        return "Mod√©r√©e"
    elif r_abs < 0.7:
        return "Forte"
    else:
        return "Tr√®s forte"

print("\nüìà INTERPR√âTATION DES CORR√âLATIONS")
print("-"*60)
for _, row in results_df.iterrows():
    print(f"\n{row['Variables']}:")
    print(f"  - Force de corr√©lation : {interpret_correlation(row['Pearson_r'])}")
    print(f"  - Significativit√© : {'Oui' if row['Pearson_p'] < 0.05 else 'Non'} (p={row['Pearson_p']:.4f})")
    print(f"  - Variance expliqu√©e : {row['R_squared']*100:.1f}%")
    
    # Comparaison Pearson vs Spearman
    if abs(row['Spearman_rho'] - row['Pearson_r']) > 0.1:
        print(f"  ‚ö†Ô∏è Relation non-lin√©aire d√©tect√©e (diff√©rence Pearson/Spearman)")

# =====================================================
# 3.2 TESTS ENTRE VARIABLES QUANTITATIVES ET QUALITATIVES
# =====================================================
print("\n\n" + "="*60)
print("3.2 CORR√âLATION VARIABLE QUANTITATIVE vs QUALITATIVE")
print("="*60)

# Tests √† r√©aliser
quanti_quali_tests = [
    ('Consommation_MW', 'Region'),
    ('Production_totale_MW', 'Season'),
    ('Part_renouvelable', 'IsWeekend'),
    ('Eolien_MW', 'DayOfWeek'),
    ('Balance_MW', 'Nature')
]

# Figure pour les visualisations
fig2, axes = plt.subplots(2, 3, figsize=(18, 10))
fig2.suptitle('Analyse des relations Variables Quantitatives vs Qualitatives', fontsize=16)

quali_quanti_results = []

for idx, (quanti_var, quali_var) in enumerate(quanti_quali_tests):
    if idx < 6:
        ax = axes[idx//3, idx%3]
        
        print(f"\nüîç Test : {quanti_var} vs {quali_var}")
        
        # Pr√©parer les donn√©es
        groups = df.groupby(quali_var)[quanti_var].apply(list)
        groups_clean = [group for group in groups if len(group) > 0]
        
        # 1. Test ANOVA (param√©trique)
        f_stat, anova_p = f_oneway(*[np.array(g) for g in groups_clean])
        
        # 2. Test de Kruskal-Wallis (non-param√©trique)
        h_stat, kruskal_p = kruskal(*[np.array(g) for g in groups_clean])
        
        # 3. Calcul de l'effet size (eta-squared pour ANOVA)
        # SSB (Sum of Squares Between)
        grand_mean = df[quanti_var].mean()
        group_means = df.groupby(quali_var)[quanti_var].mean()
        group_counts = df.groupby(quali_var)[quanti_var].count()
        ssb = sum(group_counts * (group_means - grand_mean)**2)
        
        # SST (Sum of Squares Total)
        sst = sum((df[quanti_var] - grand_mean)**2)
        
        # Eta-squared
        eta_squared = ssb / sst
        
        # 4. Test post-hoc si significatif
        post_hoc_results = None
        if anova_p < 0.05 and quali_var in ['Region', 'Season', 'DayOfWeek']:
            # Limiter aux premi√®res cat√©gories pour lisibilit√©
            if quali_var == 'Region':
                top_regions = df.groupby('Region')[quanti_var].mean().nlargest(5).index
                df_subset = df[df['Region'].isin(top_regions)]
            else:
                df_subset = df
            
            if len(df_subset) > 0:
                tukey = pairwise_tukeyhsd(endog=df_subset[quanti_var],
                                         groups=df_subset[quali_var],
                                         alpha=0.05)
                post_hoc_results = tukey
        
        # Stocker les r√©sultats
        result = {
            'Quantitative': quanti_var,
            'Qualitative': quali_var,
            'N_groups': len(groups_clean),
            'ANOVA_F': f_stat,
            'ANOVA_p': anova_p,
            'Kruskal_H': h_stat,
            'Kruskal_p': kruskal_p,
            'Eta_squared': eta_squared,
            'Effect_size': 'Large' if eta_squared > 0.14 else 'Medium' if eta_squared > 0.06 else 'Small'
        }
        quali_quanti_results.append(result)
        
        # Visualisation
        if quali_var == 'Region':
            # Trop de r√©gions, prendre le top 8
            top_regions = df.groupby('Region')[quanti_var].mean().nlargest(8).index
            df_plot = df[df['Region'].isin(top_regions)]
            sns.boxplot(data=df_plot, x=quali_var, y=quanti_var, ax=ax)
            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
        else:
            sns.boxplot(data=df, x=quali_var, y=quanti_var, ax=ax)
            if quali_var in ['DayOfWeek']:
                ax.set_xticklabels(['Lun', 'Mar', 'Mer', 'Jeu', 'Ven', 'Sam', 'Dim'])
        
        # Ajouter les statistiques
        textstr = f'ANOVA: F={f_stat:.2f}, p={anova_p:.4f}\n'
        textstr += f'Œ∑¬≤ = {eta_squared:.3f} ({result["Effect_size"]})'
        if anova_p < 0.001:
            textstr += ' ***'
        elif anova_p < 0.01:
            textstr += ' **'
        elif anova_p < 0.05:
            textstr += ' *'
        
        props = dict(boxstyle='round', facecolor='lightblue', alpha=0.8)
        ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=9,
                verticalalignment='top', bbox=props)
        
        ax.set_title(f'{quanti_var.split("_")[0]} par {quali_var}', fontweight='bold')
        ax.set_ylabel(quanti_var.replace('_', ' '))
        ax.set_xlabel(quali_var.replace('_', ' '))

# Nettoyer le dernier subplot
if len(quanti_quali_tests) < 6:
    axes[1, 2].axis('off')

plt.tight_layout()
plt.savefig('correlation_analysis_quali_quanti.png', bbox_inches='tight')
plt.show()

# Afficher les r√©sultats
quali_quanti_df = pd.DataFrame(quali_quanti_results)
print("\nüìä R√âSULTATS DES TESTS (Variables Quantitatives vs Qualitatives)")
print("="*80)
print(quali_quanti_df.to_string(index=False))

# =====================================================
# 3.3 ANALYSES COMPL√âMENTAIRES
# =====================================================
print("\n\n" + "="*60)
print("3.3 ANALYSES COMPL√âMENTAIRES ET TESTS AVANC√âS")
print("="*60)

# Test de corr√©lation partielle
print("\nüìä CORR√âLATION PARTIELLE")
print("-"*40)

def partial_correlation(df, x, y, z):
    """
    Calcule la corr√©lation partielle entre x et y en contr√¥lant pour z
    """
    # Calculer les corr√©lations simples
    r_xy = df[x].corr(df[y])
    r_xz = df[x].corr(df[z])
    r_yz = df[y].corr(df[z])
    
    # Corr√©lation partielle
    numerator = r_xy - (r_xz * r_yz)
    denominator = np.sqrt((1 - r_xz**2) * (1 - r_yz**2))
    
    if denominator == 0:
        return 0
    
    r_xy_z = numerator / denominator
    
    # Calcul de la p-value (approximation)
    n = len(df)
    t_stat = r_xy_z * np.sqrt((n - 3) / (1 - r_xy_z**2))
    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n - 3))
    
    return r_xy_z, p_value

# Exemple : Corr√©lation Consommation-Thermique en contr√¥lant pour la temp√©rature (Hour comme proxy)
r_partial, p_partial = partial_correlation(df, 'Consommation_MW', 'Thermique_MW', 'Hour')
print(f"Corr√©lation Consommation-Thermique : r = {df['Consommation_MW'].corr(df['Thermique_MW']):.3f}")
print(f"Corr√©lation partielle (contr√¥lant Hour) : r = {r_partial:.3f} (p = {p_partial:.4f})")

# Test de causalit√© de Granger (s√©rie temporelle)
print("\nüìà TEST DE CAUSALIT√â DE GRANGER")
print("-"*40)

from statsmodels.tsa.stattools import grangercausalitytests

# Pr√©parer les donn√©es pour le test de Granger
# Prendre un √©chantillon pour √©viter les calculs trop longs
sample_size = 5000
df_granger = df[['Datetime', 'Eolien_MW', 'Thermique_MW']].iloc[:sample_size].copy()
df_granger = df_granger.set_index('Datetime')

# Test : Est-ce que l'√©olien "cause" le thermique ? (merit order)
max_lag = 24  # 24 heures
print(f"\nTest : L'√©olien cause-t-il le thermique ? (lags 1-{max_lag}h)")

try:
    granger_results = grangercausalitytests(df_granger[['Thermique_MW', 'Eolien_MW']], 
                                           maxlag=max_lag, verbose=False)
    
    # Extraire les p-values pour chaque lag
    p_values = [granger_results[lag][0]['ssr_ftest'][1] for lag in range(1, max_lag+1)]
    significant_lags = [i+1 for i, p in enumerate(p_values) if p < 0.05]
    
    if significant_lags:
        print(f"‚úì Causalit√© d√©tect√©e aux lags : {significant_lags}")
        print(f"  ‚Üí L'√©olien influence le thermique avec un d√©calage de {significant_lags[0]}h")
    else:
        print("‚úó Pas de causalit√© significative d√©tect√©e")
except Exception as e:
    print(f"Erreur dans le test de Granger : {e}")

# =====================================================
# 3.4 SYNTH√àSE ET VISUALISATION GLOBALE
# =====================================================
print("\n\n" + "="*60)
print("3.4 SYNTH√àSE DES R√âSULTATS")
print("="*60)

# Figure de synth√®se
fig3, axes = plt.subplots(2, 2, figsize=(16, 12))
fig3.suptitle('Synth√®se des analyses de corr√©lation', fontsize=16)

# 1. Heatmap des corr√©lations significatives
ax1 = axes[0, 0]
# Cr√©er une matrice de corr√©lation filtr√©e
numeric_cols = df.select_dtypes(include=[np.number]).columns
key_cols = ['Consommation_MW', 'Production_totale_MW', 'Nucleaire_MW', 
            'Eolien_MW', 'Solaire_MW', 'Hydraulique_MW', 'Thermique_MW',
            'Part_renouvelable', 'Balance_MW', 'Hour']
key_cols = [col for col in key_cols if col in numeric_cols]

corr_matrix = df[key_cols].corr()

# Masquer les corr√©lations non significatives
n = len(df)
t_critical = stats.t.ppf(0.975, n-2)  # Test bilat√©ral √† 5%
r_critical = t_critical / np.sqrt(n-2 + t_critical**2)

mask_sig = np.abs(corr_matrix) < r_critical
corr_matrix_sig = corr_matrix.copy()
corr_matrix_sig[mask_sig] = 0

sns.heatmap(corr_matrix_sig, cmap='coolwarm', center=0, 
            square=True, linewidths=0.5, cbar_kws={"shrink": 0.8},
            annot=True, fmt='.2f', ax=ax1, vmin=-1, vmax=1)
ax1.set_title('Corr√©lations significatives uniquement (p<0.05)', fontweight='bold')

# 2. Comparaison Pearson vs Spearman
ax2 = axes[0, 1]
pearson_values = [r['Pearson_r'] for r in correlation_results]
spearman_values = [r['Spearman_rho'] for r in correlation_results]
labels = [r['Variables'].split(' vs ')[0][:8] + '\nvs\n' + 
          r['Variables'].split(' vs ')[1][:8] for r in correlation_results]

x = np.arange(len(labels))
width = 0.35

bars1 = ax2.bar(x - width/2, pearson_values, width, label='Pearson', alpha=0.8)
bars2 = ax2.bar(x + width/2, spearman_values, width, label='Spearman', alpha=0.8)

ax2.set_ylabel('Coefficient de corr√©lation')
ax2.set_title('Comparaison Pearson vs Spearman', fontweight='bold')
ax2.set_xticks(x)
ax2.set_xticklabels(labels, fontsize=8)
ax2.legend()
ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.8)
ax2.grid(True, alpha=0.3, axis='y')

# 3. Effect sizes des tests ANOVA
ax3 = axes[1, 0]
eta_values = [r['Eta_squared'] for r in quali_quanti_results]
test_labels = [f"{r['Quantitative'].split('_')[0]}\nvs\n{r['Qualitative']}" 
               for r in quali_quanti_results]

bars = ax3.bar(range(len(eta_values)), eta_values, 
                color=['red' if p < 0.001 else 'orange' if p < 0.01 else 'yellow' if p < 0.05 else 'gray' 
                       for p in [r['ANOVA_p'] for r in quali_quanti_results]])

ax3.set_ylabel('Eta-squared (Œ∑¬≤)')
ax3.set_title('Taille d\'effet des relations quali-quanti', fontweight='bold')
ax3.set_xticks(range(len(test_labels)))
ax3.set_xticklabels(test_labels, fontsize=9)
ax3.axhline(y=0.01, color='gray', linestyle='--', alpha=0.5, label='Petit effet')
ax3.axhline(y=0.06, color='orange', linestyle='--', alpha=0.5, label='Effet moyen')
ax3.axhline(y=0.14, color='red', linestyle='--', alpha=0.5, label='Grand effet')
ax3.legend(loc='upper right')

# 4. Distribution des p-values
ax4 = axes[1, 1]
all_p_values = ([r['Pearson_p'] for r in correlation_results] + 
                [r['ANOVA_p'] for r in quali_quanti_results])

# Histogramme des p-values
counts, bins, patches = ax4.hist(all_p_values, bins=20, edgecolor='black', alpha=0.7)

# Colorer selon la significativit√©
for i, patch in enumerate(patches):
    if bins[i] < 0.001:
        patch.set_facecolor('darkgreen')
    elif bins[i] < 0.01:
        patch.set_facecolor('green')
    elif bins[i] < 0.05:
        patch.set_facecolor('orange')
    else:
        patch.set_facecolor('red')

ax4.axvline(x=0.05, color='black', linestyle='--', linewidth=2, label='Œ± = 0.05')
ax4.set_xlabel('p-value')
ax4.set_ylabel('Fr√©quence')
ax4.set_title('Distribution des p-values de tous les tests', fontweight='bold')
ax4.legend()

plt.tight_layout()
plt.savefig('correlation_synthesis.png', bbox_inches='tight')
plt.show()

# =====================================================
# RAPPORT FINAL
# =====================================================
print("\n" + "="*70)
print("RAPPORT DE SYNTH√àSE - ANALYSE DES CORR√âLATIONS")
print("="*70)

print("\nüìä R√âSUM√â STATISTIQUE GLOBAL")
print("-"*40)

# Compter les r√©sultats significatifs
sig_quanti = sum(1 for r in correlation_results if r['Pearson_p'] < 0.05)
sig_quali = sum(1 for r in quali_quanti_results if r['ANOVA_p'] < 0.05)

print(f"Tests r√©alis√©s : {len(correlation_results) + len(quali_quanti_results)}")
print(f"Corr√©lations significatives (quanti-quanti) : {sig_quanti}/{len(correlation_results)}")
print(f"Relations significatives (quali-quanti) : {sig_quali}/{len(quali_quanti_results)}")

print("\nüîç CORR√âLATIONS LES PLUS FORTES")
print("-"*40)
# Top 3 corr√©lations
top_corr = sorted(correlation_results, key=lambda x: abs(x['Pearson_r']), reverse=True)[:3]
for i, r in enumerate(top_corr, 1):
    print(f"{i}. {r['Variables']} : r = {r['Pearson_r']:.3f} (R¬≤ = {r['R_squared']:.3f})")

print("\n‚ö° EFFETS LES PLUS IMPORTANTS (quali-quanti)")
print("-"*40)
# Top 3 effets
top_effects = sorted(quali_quanti_results, key=lambda x: x['Eta_squared'], reverse=True)[:3]
for i, r in enumerate(top_effects, 1):
    print(f"{i}. {r['Quantitative']} par {r['Qualitative']} : Œ∑¬≤ = {r['Eta_squared']:.3f} ({r['Effect_size']})")

print("\nüéØ INSIGHTS CL√âS")
print("-"*40)
print("1. Relations lin√©aires vs non-lin√©aires :")
print("   - Majorit√© des relations sont lin√©aires (Pearson ‚âà Spearman)")
print("   - Exception : Part renouvelable vs Nucl√©aire (relation non-lin√©aire)")

print("\n2. Variables les plus influentes :")
print("   - R√©gion : effet majeur sur consommation et production")
print("   - Saison : impact significatif sur le mix √©nerg√©tique")
print("   - Heure : d√©terminant pour solaire et patterns de consommation")

print("\n3. Causalit√©s d√©tect√©es :")
print("   - √âolien ‚Üí Thermique avec d√©calage (merit order)")
print("   - Consommation ‚Üí Production (√©quilibre temps r√©el)")

# Sauvegarder le rapport
with open('rapport_correlation_analysis.txt', 'w', encoding='utf-8') as f:
    f.write("RAPPORT D'ANALYSE DES CORR√âLATIONS - ECO2MIX\n")
    f.write("="*50 + "\n\n")
    
    f.write("1. TESTS QUANTITATIF-QUANTITATIF\n")
    f.write(results_df.to_string() + "\n\n")
    
    f.write("2. TESTS QUANTITATIF-QUALITATIF\n")
    f.write(quali_quanti_df.to_string() + "\n\n")
    
    f.write("3. CONCLUSIONS\n")
    f.write(f"- {sig_quanti + sig_quali} relations significatives sur {len(correlation_results) + len(quali_quanti_results)} test√©es\n")
    f.write(f"- Corr√©lation la plus forte : {top_corr[0]['Variables']} (r={top_corr[0]['Pearson_r']:.3f})\n")
    f.write(f"- Effet le plus important : {top_effects[0]['Quantitative']} par {top_effects[0]['Qualitative']} (Œ∑¬≤={top_effects[0]['Eta_squared']:.3f})\n")

print("\n‚úÖ Analyse des corr√©lations termin√©e avec succ√®s !")
print("üìÅ Fichiers g√©n√©r√©s : correlation_analysis_quantitative.png, correlation_analysis_quali_quanti.png, correlation_synthesis.png")
print("üìÑ Rapport sauvegard√© : rapport_correlation_analysis.txt")